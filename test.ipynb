{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e264a5b7-a3d4-489f-91a2-350b06415bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"]=\"AIzaSyDwatTqd6cd6DczbAKq6ZLAr-8Vj0vKSio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a777c676-5c98-48d4-975b-7ba7c10e4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb06a61b-effb-4e56-a6ab-084bd712ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Description 1 - Name 1\",\n",
    "        metadata={\"id\": \"1\", \"name\": \"Name 1\", \"link\": \"http://example.com/1\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Description 2 - Name 2\",\n",
    "        metadata={\"id\": \"2\", \"name\": \"Name 2\", \"link\": \"http://example.com/2\"}\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d49e0cc0-d9cc-4e96-b66d-18eed7d05fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FAISS vector store from documentsArithmeticError\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5448c96-c4b2-4f6d-b01b-00b42c60d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9002bab2-0c3a-4f5b-9a1c-01687faaba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vector store from the 'faiss_index' directory\n",
    "new_vector_store = FAISS.load_local(\n",
    "    \"faiss_index\", embeddings, allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd7676f0-b000-4cef-a054-3536da4ff1fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'FAISS' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(new_vector_store)\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'FAISS' has no len()"
     ]
    }
   ],
   "source": [
    "len(new_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676dc52f-0cc7-42d7-aef5-16ec04abaed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74669ae-5869-4448-b688-54d11ed1678c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8146fca8-515d-4df4-9d66-78e709e5f01c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d11b93-3798-437c-b7d0-51033d11eae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f3311e-0f3f-4281-a1c6-185ba359653e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc90ea2-d592-42dc-8de8-f0dcd1bbb628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a69932d-defc-4257-b3d6-3490ea0d9175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532000ea-33f9-435a-b3ce-57505aa43a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679686dc-1214-465e-8b34-70d0a73a56cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb37a58e-0dfa-4de6-9fe9-6715d61c8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBdXiAZt4JvPAkElWdT1CPMMaZJm9TkQ04\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7489187-dbd7-4247-86f8-88cc8f7f9d5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Initialize vector store (run this once)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m pdf_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpastpapers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 66\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m create_vector_store(pdf_folder, api_key)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Load existing vector store (for subsequent runs)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m GoogleGenerativeAIEmbeddings(\n\u001b[0;32m     70\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/embedding-001\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'api_key' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain_core.retrievers import VectorStoreRetriever\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import glob\n",
    "\n",
    "def create_vector_store(pdf_folder_path: str, api_key: str):\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/embedding-001\",\n",
    "        api_key=api_key\n",
    "    )\n",
    "    \n",
    "    # Initialize documents list\n",
    "    documents = []\n",
    "    \n",
    "    # Get all PDF files from the folder\n",
    "    pdf_files = glob.glob(os.path.join(pdf_folder_path, \"**/*.pdf\"), recursive=True)\n",
    "    \n",
    "    # Process each PDF file\n",
    "    for pdf_path in pdf_files:\n",
    "        try:\n",
    "            # Extract the relative path for metadata\n",
    "            relative_path = os.path.relpath(pdf_path, pdf_folder_path)\n",
    "            \n",
    "            # Read PDF content\n",
    "            pdf_reader = PdfReader(pdf_path)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "            \n",
    "            # Create document with metadata\n",
    "            doc = Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"source\": relative_path,\n",
    "                    \"file_path\": pdf_path,\n",
    "                    \"file_name\": os.path.basename(pdf_path)\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Create and save vector store\n",
    "    vector_store = FAISS.from_documents(splits, embeddings)\n",
    "    vector_store.save_local(\"faiss_past_papers\")\n",
    "    \n",
    "    return vector_store\n",
    "    \n",
    "# Initialize vector store (run this once)\n",
    "pdf_folder = \"pastpapers\"\n",
    "vector_store = create_vector_store(pdf_folder, api_key)\n",
    "\n",
    "# Load existing vector store (for subsequent runs)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\"\n",
    ")\n",
    "vector_store = FAISS.load_local(\"faiss_past_papers\", embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af0119-65d1-4a93-9950-9dd3fcb04e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain_core.retrievers import VectorStoreRetriever\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "import os\n",
    "\n",
    "# Initialize Google Generative AI model and embeddings\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Index past paper PDFs into a vector store (run once to initialize the index)\n",
    "def index_pdfs(folder_path, vector_store):\n",
    "    pdf_documents = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    content = f.read()\n",
    "                pdf_documents.append(Document(page_content=content.decode(errors='ignore'), metadata={\"path\": file_path}))\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(pdf_documents)\n",
    "    vector_store.add_documents(documents=chunks)\n",
    "\n",
    "# Define vector store (FAISS used here for local storage)\n",
    "vector_store = FAISS.load_local(\"faiss_links_only\", embeddings)\n",
    "\n",
    "# Define state for the application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    retrieved_docs = retriever.retrieve(state[\"question\"], k=5)\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    prompt_template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are a helpful assistant providing past paper links.\"),\n",
    "        (\"user\", \"Find me past papers for: {question}\"),\n",
    "    ])\n",
    "    prompt = prompt_template.invoke({\"question\": state[\"question\"]})\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Compile application graph\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Example Query (You can integrate this into a bot interface)\n",
    "def handle_query(user_query):\n",
    "    state = {\"question\": user_query, \"context\": [], \"answer\": \"\"}\n",
    "    updated_state = graph.run(state)\n",
    "    return updated_state[\"answer\"]\n",
    "\n",
    "# Uncomment below lines to initialize the vector store with your PDFs (only needed once)\n",
    "# index_pdfs(\"path_to_your_pdfs\", vector_store)\n",
    "\n",
    "# Example usage\n",
    "# print(handle_query(\"1999 Physics SL May Examination Session English\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d81ef-7013-49c8-9a65-896dd343e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Initialize Google Generative AI model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.5)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector_store = FAISS.load_local(\n",
    "    \"faiss_past_papers/\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "def debug_metadata(documents):\n",
    "    \"\"\"Print metadata of retrieved documents to ensure paths are set correctly.\"\"\"\n",
    "    for i, doc in enumerate(documents):\n",
    "        print(f\"Document {i+1} Metadata: {doc.metadata}\")\n",
    "\n",
    "def create_pdf_links_html(documents):\n",
    "    \"\"\"Creates an HTML representation of document summaries and links.\"\"\"\n",
    "    html_content = \"<div style='margin-bottom: 20px;'>\"\n",
    "    for i, doc in enumerate(documents):\n",
    "        pdf_path = doc.metadata.get('file_path', '')  # Use 'file_path' metadata\n",
    "        content_summary = doc.page_content  # First 200 characters as a summary\n",
    "        file_uri = 'file:///' + pdf_path.replace('\\\\', '/').lstrip('/')\n",
    "        html_content += f\"\"\"\n",
    "            <div style='padding: 10px; border-bottom: 1px solid #eee;'>\n",
    "                <h4>Document {i+1}</h4>\n",
    "                <p>{content_summary}...</p>\n",
    "                <p><strong>Source:</strong> <a href='{file_uri}' target='_blank'>{pdf_path}</a></p>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "    html_content += \"</div>\"\n",
    "    return html_content\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    html_links: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    \"\"\"Retrieve multiple relevant documents based on the user's query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"],k=20)  \n",
    "    if not retrieved_docs:\n",
    "        return {\"context\": [], \"html_links\": \"\"}\n",
    "    \n",
    "    # Generate HTML links for all retrieved documents\n",
    "    html_links = create_pdf_links_html(retrieved_docs)\n",
    "    return {\"context\": retrieved_docs, \"html_links\": html_links}\n",
    "\n",
    "# Detailed prompt template with example response\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"\"\"You are a helpful assistant tasked with finding relevant documents for users. For every query:\n",
    "1. Summarize each document in 1-2 sentences.\n",
    "2. Include clickable links for users to access the documents.\n",
    "3. Avoid unnecessary details â€” focus only on what's relevant to the query.\n",
    "\n",
    "Example Response:\n",
    "Question: \"Please provide Physics SL papers from 2021.\"\n",
    "\n",
    "Response:\n",
    "1. Document 1: This contains the Physics SL paper from May 2021, including questions and mark schemes. [Link to Document](file:///path/to/document1.pdf)\n",
    "2. Document 2: A supplementary guide for Physics SL students, covering exam preparation techniques. [Link to Document](file:///path/to/document2.pdf)\"\"\"),\n",
    "    (\"user\", \"\"\"Question: {question}\n",
    "\n",
    "Relevant Documents:\n",
    "{document_details}\n",
    "\n",
    "Provide a concise summary of each document along with its link.\"\"\")\n",
    "])\n",
    "\n",
    "def generate(state: State):\n",
    "    \"\"\"Generate a response summarizing all retrieved documents.\"\"\"\n",
    "    if not state[\"context\"]:\n",
    "        return {\"answer\": \"Sorry, I couldn't find any documents matching your query.\"}\n",
    "    \n",
    "    # Build document details for all retrieved documents\n",
    "    document_details = \"\"\n",
    "    for i, doc in enumerate(state[\"context\"]):\n",
    "        content_summary = doc.page_content\n",
    "        pdf_path = doc.metadata.get('file_path', '')\n",
    "        document_details += f\"Document {i+1}: \\nSummary: {content_summary}...\\nLink: {pdf_path}\\n\\n\"\n",
    "\n",
    "    # Create a prompt using the document details\n",
    "    prompt = prompt_template.invoke({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"document_details\": document_details,\n",
    "    })\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Define the application graph\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"retrieve\", retrieve)\n",
    "graph_builder.add_node(\"generate\", generate)\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_builder.add_edge(\"retrieve\", \"generate\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "def handle_query(user_query):\n",
    "    \"\"\"Handles the user query and returns a styled HTML response.\"\"\"\n",
    "    state = {\n",
    "        \"question\": user_query,\n",
    "        \"context\": [],\n",
    "        \"answer\": \"\",\n",
    "        \"html_links\": \"\"\n",
    "    }\n",
    "    updated_state = graph.invoke(state)\n",
    "    \n",
    "    return HTML(f\"\"\"\n",
    "        <div style='font-family: Arial, sans-serif; line-height: 1.6;'>\n",
    "            <div style='background-color: #f9f9f9; padding: 15px; border-radius: 8px; margin-bottom: 20px;'>\n",
    "                <h3 style='color: #2c3e50;'>ðŸ¤– Here's what I found for you:</h3>\n",
    "                <p>{updated_state['answer']}</p>\n",
    "            </div>\n",
    "            <div>\n",
    "                <h3 style='color: #2c3e50;'>ðŸ“‚ Related Documents:</h3>\n",
    "                {updated_state['html_links']}\n",
    "            </div>\n",
    "        </div>\n",
    "    \"\"\")\n",
    "\n",
    "# Example usage:\n",
    "display(handle_query(\"Please provide Physics SL papers from 2021\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c1720-95c9-4a14-b65d-78fa37dec2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=100,\n",
    ")\n",
    "google_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "new_db = FAISS.load_local(\"faiss_past_papers/\", google_embeddings,allow_dangerous_deserialization=True)\n",
    "\n",
    "def getSimilar_documents(query):\n",
    "    relevant_docs = new_db.similarity_search(query,k=20)\n",
    "    \n",
    "    # Convert documents into the desired format\n",
    "    documents = []\n",
    "    for doc in relevant_docs:\n",
    "        meta = doc.metadata.get('file_path', '').replace('\\\\', '/')  # Extract metadata\n",
    "        content_preview = doc.page_content[:200]  # Extract first 200 characters of content\n",
    "        document = {\n",
    "            \"metadata\": meta,\n",
    "            \"content_preview\": content_preview\n",
    "        }\n",
    "        documents.append(document)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def generate_response(query):\n",
    "    context=getSimilar_documents(query)\n",
    "    context=json.dumps(context, indent=2)\n",
    "    prompt_template = \"\"\"\n",
    "    You are an intelligent assistant designed to assist users in finding past paper links based on their queries. Your task is to:\n",
    "    Analyze the provided document objects containing all data (metadata and content) for each paper.\n",
    "    \n",
    "    documents: {context}\n",
    "    \n",
    "    - Use the metadata and the first 200 characters of the content to understand the documentâ€™s subject, type, and relevant details.\n",
    "    - Analyze the user query to understand the specific past paper being requested (e.g., subject, year, type).\n",
    "    - Compare the user query against the metadata and content previews within the document objects provided.\n",
    "    - Identify the document that best matches the query.\n",
    "    - Provide the document link and a brief explanation of the documentâ€™s relevance based on its metadata and content.\n",
    "    \n",
    "    If a match is found:\n",
    "    \n",
    "    Provide the documentâ€™s link.\n",
    "    \n",
    "    Summarize the documentâ€™s content based on the metadata and content.\n",
    "    \n",
    "    If no match is found, politely inform the user that no relevant document was found based on their query.\n",
    "    \n",
    "    Always provide clear, concise, and helpful responses\n",
    "    \n",
    "    user query: {question}\n",
    "    Output Format:\n",
    "    \n",
    "    \n",
    "    Here's everything related to [user's query or topic]\n",
    "\n",
    "Below are the relevant documents, along with their links and a brief description:\n",
    "\n",
    "1. [Document Title 1 (make some title based on pdf file name)]\n",
    "ðŸ”— [Link to Document 1]\n",
    "Overview: A detailed analysis of [brief description of the document's content, e.g., kinematics and dynamics in Physics 2022].\n",
    "\n",
    "2. [Document Title 2 (make some title based on pdf file name)]\n",
    "ðŸ”— [Link to Document 2]\n",
    "Overview: This document discusses [short summary based on the document content, e.g., thermodynamics principles and applications].\n",
    "\n",
    "3. [Document Title 3 (make some title based on pdf file name)]\n",
    "ðŸ”— [Link to Document 3]\n",
    "Overview: Contains comprehensive information on [brief description of content, e.g., Newtonâ€™s laws and their real-world applications].\n",
    "    \n",
    "    If no match is found:\n",
    "    \n",
    "    Response:\n",
    "    \"I couldn't find a document matching your query. Please provide more details, such as the subject, year, or type of paper you're looking for.\"\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    PROMPT = PromptTemplate(\n",
    "            template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    chain = PROMPT | llm | output_parser\n",
    "\n",
    "    response = chain.invoke({\"context\": context, \"question\": query})\n",
    "    print(response)\n",
    "\n",
    "generate_response(\"Find Physics SL papers from 2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc5725-d3cd-4c95-aee0-d21c0751a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilar_documents(query):\n",
    "    relevant_docs = new_db.similarity_search(query,k=20)\n",
    "    \n",
    "    # Convert documents into the desired format\n",
    "    documents = []\n",
    "    for doc in relevant_docs:\n",
    "        meta = doc.metadata.get('file_path', '').replace('\\\\', '/')  # Extract metadata\n",
    "        content_preview = doc.page_content[:200]  # Extract first 200 characters of content\n",
    "        document = {\n",
    "            \"metadata\": meta,\n",
    "            \"content_preview\": content_preview\n",
    "        }\n",
    "        documents.append(document)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "context=getSimilar_documents(\"Find Physics SL papers from 2021\")\n",
    "strr=json.dumps(context, indent=2)\n",
    "strr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1aac05-d014-4458-87ca-68f65ed6c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install O365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8a5cca7-4404-4133-8cee-34ff2289eb7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visit the following url to give consent:\n",
      "https://login.microsoftonline.com/common/oauth2/v2.0/authorize?response_type=code&client_id=be9fab77-1cd5-45c1-91f3-81371dfae656&redirect_uri=https%3A%2F%2Flogin.microsoftonline.com%2Fcommon%2Foauth2%2Fnativeclient&scope=Files.Read.All+Sites.Read.All+offline_access+Files.Read&state=dSamLZoHtps1q26YbpfmjArmkvIzDR&access_type=offline&code=M.C529_BAY.2.U.1ed9694d-c6bf-7cbe-d3f4-3b756b3575af\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Paste the authenticated url here:\n",
      " https://login.microsoftonline.com/common/oauth2/nativeclient?code=M.C529_BAY.2.U.93c15d3c-ed5e-fe43-a555-d27202e280f2&state=dSamLZoHtps1q26YbpfmjArmkvIzDR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication Flow Completed. Oauth Access Token Stored. You can now use the API.\n",
      "Authentication successful!\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for _O365TokenStorage\ntoken_path\n  Path does not point to a file [type=path_not_file, input_value=WindowsPath('C:/Users/Abb...entials/o365_token.txt'), input_type=WindowsPath]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 50\u001b[0m\n\u001b[0;32m     43\u001b[0m loader \u001b[38;5;241m=\u001b[39m OneDriveLoader(\n\u001b[0;32m     44\u001b[0m     drive_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2c96b4e8cbcb67b5\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     45\u001b[0m     folder_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmirEjaz\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     46\u001b[0m     auth_with_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Load documents\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m documents \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(documents)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:31\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy_load())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\document_loaders\\sharepoint.py:57\u001b[0m, in \u001b[0;36mSharePointLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO365 package not found, please install it with `pip install o365`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m     )\n\u001b[1;32m---> 57\u001b[0m drive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auth()\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39mget_drive(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_library_id)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(drive, Drive):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt a Drive with id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_library_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\document_loaders\\base_o365.py:278\u001b[0m, in \u001b[0;36mO365BaseLoader._auth\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO365 package not found, please install it with `pip install o365`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     )\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth_with_token:\n\u001b[1;32m--> 278\u001b[0m     token_storage \u001b[38;5;241m=\u001b[39m _O365TokenStorage()\n\u001b[0;32m    279\u001b[0m     token_path \u001b[38;5;241m=\u001b[39m token_storage\u001b[38;5;241m.\u001b[39mtoken_path\n\u001b[0;32m    280\u001b[0m     token_backend \u001b[38;5;241m=\u001b[39m FileSystemTokenBackend(\n\u001b[0;32m    281\u001b[0m         token_path\u001b[38;5;241m=\u001b[39mtoken_path\u001b[38;5;241m.\u001b[39mparent, token_filename\u001b[38;5;241m=\u001b[39mtoken_path\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    282\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic_settings\\main.py:167\u001b[0m, in \u001b[0;36mBaseSettings.__init__\u001b[1;34m(__pydantic_self__, _case_sensitive, _nested_model_default_partial_update, _env_prefix, _env_file, _env_file_encoding, _env_ignore_empty, _env_nested_delimiter, _env_parse_none_str, _env_parse_enums, _cli_prog_name, _cli_parse_args, _cli_settings_source, _cli_parse_none_str, _cli_hide_none_type, _cli_avoid_json, _cli_enforce_required, _cli_use_class_docs_for_groups, _cli_exit_on_error, _cli_prefix, _cli_flag_prefix_char, _cli_implicit_flags, _cli_ignore_unknown_args, _secrets_dir, **values)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    140\u001b[0m     __pydantic_self__,\n\u001b[0;32m    141\u001b[0m     _case_sensitive: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m__pydantic_self__\u001b[38;5;241m.\u001b[39m_settings_build_values(\n\u001b[0;32m    169\u001b[0m             values,\n\u001b[0;32m    170\u001b[0m             _case_sensitive\u001b[38;5;241m=\u001b[39m_case_sensitive,\n\u001b[0;32m    171\u001b[0m             _nested_model_default_partial_update\u001b[38;5;241m=\u001b[39m_nested_model_default_partial_update,\n\u001b[0;32m    172\u001b[0m             _env_prefix\u001b[38;5;241m=\u001b[39m_env_prefix,\n\u001b[0;32m    173\u001b[0m             _env_file\u001b[38;5;241m=\u001b[39m_env_file,\n\u001b[0;32m    174\u001b[0m             _env_file_encoding\u001b[38;5;241m=\u001b[39m_env_file_encoding,\n\u001b[0;32m    175\u001b[0m             _env_ignore_empty\u001b[38;5;241m=\u001b[39m_env_ignore_empty,\n\u001b[0;32m    176\u001b[0m             _env_nested_delimiter\u001b[38;5;241m=\u001b[39m_env_nested_delimiter,\n\u001b[0;32m    177\u001b[0m             _env_parse_none_str\u001b[38;5;241m=\u001b[39m_env_parse_none_str,\n\u001b[0;32m    178\u001b[0m             _env_parse_enums\u001b[38;5;241m=\u001b[39m_env_parse_enums,\n\u001b[0;32m    179\u001b[0m             _cli_prog_name\u001b[38;5;241m=\u001b[39m_cli_prog_name,\n\u001b[0;32m    180\u001b[0m             _cli_parse_args\u001b[38;5;241m=\u001b[39m_cli_parse_args,\n\u001b[0;32m    181\u001b[0m             _cli_settings_source\u001b[38;5;241m=\u001b[39m_cli_settings_source,\n\u001b[0;32m    182\u001b[0m             _cli_parse_none_str\u001b[38;5;241m=\u001b[39m_cli_parse_none_str,\n\u001b[0;32m    183\u001b[0m             _cli_hide_none_type\u001b[38;5;241m=\u001b[39m_cli_hide_none_type,\n\u001b[0;32m    184\u001b[0m             _cli_avoid_json\u001b[38;5;241m=\u001b[39m_cli_avoid_json,\n\u001b[0;32m    185\u001b[0m             _cli_enforce_required\u001b[38;5;241m=\u001b[39m_cli_enforce_required,\n\u001b[0;32m    186\u001b[0m             _cli_use_class_docs_for_groups\u001b[38;5;241m=\u001b[39m_cli_use_class_docs_for_groups,\n\u001b[0;32m    187\u001b[0m             _cli_exit_on_error\u001b[38;5;241m=\u001b[39m_cli_exit_on_error,\n\u001b[0;32m    188\u001b[0m             _cli_prefix\u001b[38;5;241m=\u001b[39m_cli_prefix,\n\u001b[0;32m    189\u001b[0m             _cli_flag_prefix_char\u001b[38;5;241m=\u001b[39m_cli_flag_prefix_char,\n\u001b[0;32m    190\u001b[0m             _cli_implicit_flags\u001b[38;5;241m=\u001b[39m_cli_implicit_flags,\n\u001b[0;32m    191\u001b[0m             _cli_ignore_unknown_args\u001b[38;5;241m=\u001b[39m_cli_ignore_unknown_args,\n\u001b[0;32m    192\u001b[0m             _secrets_dir\u001b[38;5;241m=\u001b[39m_secrets_dir,\n\u001b[0;32m    193\u001b[0m         )\n\u001b[0;32m    194\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_python(data, self_instance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for _O365TokenStorage\ntoken_path\n  Path does not point to a file [type=path_not_file, input_value=WindowsPath('C:/Users/Abb...entials/o365_token.txt'), input_type=WindowsPath]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from O365 import Account\n",
    "from langchain_community.document_loaders.onedrive import OneDriveLoader\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "# Set environment variables for authentication\n",
    "os.environ['O365_CLIENT_ID'] = 'be9fab77-1cd5-45c1-91f3-81371dfae656'\n",
    "os.environ['O365_CLIENT_SECRET'] = '~jw8Q~BhZBf~jGVA1rVXfFeBRrijZRneNWcbwdoA'\n",
    "\n",
    "scopes = [\n",
    "    'offline_access',\n",
    "    'Files.Read',\n",
    "    'Files.Read.All',\n",
    "    'Sites.Read.All'\n",
    "]\n",
    "\n",
    "# Token file path (ensure the file exists or create it)\n",
    "token_path = \"C:/Users/Abbas/Desktop/pp project test/credentials/o365_token.txt\"\n",
    "\n",
    "# Verify if the token file exists and is a valid file\n",
    "if not os.path.isfile(token_path):\n",
    "    raise FileNotFoundError(f\"The token file does not exist or is not a valid file: {token_path}\")\n",
    "\n",
    "# Initialize the account with credentials and scopes\n",
    "credentials = (os.environ['O365_CLIENT_ID'], os.environ['O365_CLIENT_SECRET'])\n",
    "account = Account(credentials, token_path=token_path, scopes=scopes)\n",
    "\n",
    "# Get the URL that you received after authentication (replace this with your actual URL)\n",
    "auth_url = 'http://localhost:5000/?code=M.C529_BAY.2.U.1ed9694d-c6bf-7cbe-d3f4-3b756b3575af&state=S13Ro0ZDO4HjTPOb7RD0WA'\n",
    "\n",
    "# Parse the URL and extract the authorization code\n",
    "parsed_url = urlparse(auth_url)\n",
    "code = parse_qs(parsed_url.query).get('code', [None])[0]\n",
    "\n",
    "if code:\n",
    "    # Now authenticate using the authorization code (no need to pass scopes here)\n",
    "    account.authenticate(code=code)\n",
    "    print(\"Authentication successful!\")\n",
    "else:\n",
    "    print(\"Missing code in the URL.\")\n",
    "\n",
    "# Initialize the OneDriveLoader\n",
    "loader = OneDriveLoader(\n",
    "    drive_id='2C96B4E8CBCB67B5',\n",
    "    folder_path='AmirEjaz',\n",
    "    auth_with_token=True\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "documents = loader.load()\n",
    "print(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4392a6a-7e9b-4181-a16d-79b9e22b748e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visit the following url to give consent:\n",
      "https://login.microsoftonline.com/common/oauth2/v2.0/authorize?response_type=code&client_id=be9fab77-1cd5-45c1-91f3-81371dfae656&redirect_uri=https%3A%2F%2Flogin.microsoftonline.com%2Fcommon%2Foauth2%2Fnativeclient&scope=https%3A%2F%2Fgraph.microsoft.com%2FSites.Read.All+offline_access+https%3A%2F%2Fgraph.microsoft.com%2FUser.Read&state=bguXwgrJj7hVZyTeC7zpYFHcIfXMCX&access_type=offline\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Paste the authenticated url here:\n",
      " https://login.microsoftonline.com/common/oauth2/nativeclient?code=M.C529_BAY.2.U.8ddc80c7-92fd-af4a-a8eb-5f9763cb68f5&state=bguXwgrJj7hVZyTeC7zpYFHcIfXMCX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication Flow Completed. Oauth Access Token Stored. You can now use the API.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from O365 import Account\n",
    "from langchain_community.document_loaders.onedrive import OneDriveLoader\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "# Set environment variables for authentication\n",
    "os.environ['O365_CLIENT_ID'] = 'be9fab77-1cd5-45c1-91f3-81371dfae656'\n",
    "os.environ['O365_CLIENT_SECRET'] = '~jw8Q~BhZBf~jGVA1rVXfFeBRrijZRneNWcbwdoA'  # Replace with your secret\n",
    "\n",
    "from langchain_community.document_loaders.onedrive import OneDriveLoader\n",
    "\n",
    "loader = OneDriveLoader(drive_id=\"2C96B4E8CBCB67B5\")\n",
    "documents = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b096370-23f0-4c2b-80e9-903eddcfe7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.onedrive import OneDriveLoader\n",
    "\n",
    "loader = OneDriveLoader(drive_id=\"2C96B4E8CBCB67B5\", object_ids=[\"2C96B4E8CBCB67B5!sa936ad78ad8c4432854be3a7831a93d7\"], auth_with_token=True\n",
    "                       ,recursive=True)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dcbb19d9-a280-4589-9230-a697b9b961bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "891a9451-d933-4ac8-b1d6-ad380773b64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.12.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.8 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index) (0.12.8)\n",
      "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.3.12-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.4.1-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
      "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.4.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.52.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.8->llama-index) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (2024.3.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (3.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (10.3.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (8.2.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (1.14.1)\n",
      "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud-0.1.7-py3-none-any.whl.metadata (860 bytes)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
      "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Using cached pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.5.19-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: click in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (2023.10.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.8->llama-index) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.8->llama-index) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.8->llama-index) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.8->llama-index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.8->llama-index) (1.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.5)\n",
      "Collecting certifi<2025.0.0,>=2024.7.4 (from llama-cloud>=0.1.5->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.8->llama-index) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.8->llama-index) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.8->llama-index) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.8->llama-index) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.8->llama-index) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from click->nltk>3.8.1->llama-index) (0.4.6)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.6.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.8->llama-index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.8->llama-index) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.8->llama-index) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.8->llama-index) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.8->llama-index) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.8->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.8->llama-index) (3.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.8->llama-index) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abbas\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.16.0)\n",
      "Downloading llama_index-0.12.8-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_llms_openai-0.3.12-py3-none-any.whl (14 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.4.1-py3-none-any.whl (5.8 kB)\n",
      "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.4.1-py3-none-any.whl (38 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Downloading llama_cloud-0.1.7-py3-none-any.whl (242 kB)\n",
      "   ---------------------------------------- 0.0/242.1 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/242.1 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/242.1 kB 325.1 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 61.4/242.1 kB 409.6 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 71.7/242.1 kB 393.8 kB/s eta 0:00:01\n",
      "   -------------- ------------------------ 92.2/242.1 kB 435.7 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 102.4/242.1 kB 368.6 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 133.1/242.1 kB 413.7 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 153.6/242.1 kB 416.7 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 153.6/242.1 kB 416.7 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 184.3/242.1 kB 397.4 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 204.8/242.1 kB 414.8 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 215.0/242.1 kB 385.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- 242.1/242.1 kB 390.3 kB/s eta 0:00:00\n",
      "Downloading llama_parse-0.5.19-py3-none-any.whl (15 kB)\n",
      "Downloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
      "   ---------------------------------------- 0.0/454.3 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/454.3 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/454.3 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 30.7/454.3 kB 330.3 kB/s eta 0:00:02\n",
      "   ----- --------------------------------- 61.4/454.3 kB 365.7 kB/s eta 0:00:02\n",
      "   ------ -------------------------------- 71.7/454.3 kB 328.6 kB/s eta 0:00:02\n",
      "   ------- ------------------------------- 92.2/454.3 kB 350.1 kB/s eta 0:00:02\n",
      "   ------- ------------------------------- 92.2/454.3 kB 350.1 kB/s eta 0:00:02\n",
      "   ---------- --------------------------- 122.9/454.3 kB 343.4 kB/s eta 0:00:01\n",
      "   ----------- -------------------------- 143.4/454.3 kB 355.0 kB/s eta 0:00:01\n",
      "   ------------ ------------------------- 153.6/454.3 kB 339.7 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 174.1/454.3 kB 349.3 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 184.3/454.3 kB 338.0 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 204.8/454.3 kB 346.2 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 225.3/454.3 kB 362.0 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 235.5/454.3 kB 352.0 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 256.0/454.3 kB 365.7 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 286.7/454.3 kB 368.6 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 286.7/454.3 kB 368.6 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 286.7/454.3 kB 368.6 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 317.4/454.3 kB 345.2 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 337.9/454.3 kB 361.7 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 348.2/454.3 kB 348.7 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 348.2/454.3 kB 348.7 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 389.1/454.3 kB 351.5 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 389.1/454.3 kB 351.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 399.4/454.3 kB 336.6 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 419.8/454.3 kB 340.5 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 430.1/454.3 kB 335.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- 454.3/454.3 kB 338.4 kB/s eta 0:00:00\n",
      "Using cached pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "   ---------------------------------------- 0.0/164.9 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/164.9 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/164.9 kB ? eta -:--:--\n",
      "   --------- ----------------------------- 41.0/164.9 kB 388.9 kB/s eta 0:00:01\n",
      "   -------------- ------------------------ 61.4/164.9 kB 465.5 kB/s eta 0:00:01\n",
      "   ---------------- ---------------------- 71.7/164.9 kB 391.3 kB/s eta 0:00:01\n",
      "   ---------------- ---------------------- 71.7/164.9 kB 391.3 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 112.6/164.9 kB 363.1 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 122.9/164.9 kB 399.4 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 143.4/164.9 kB 369.8 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 143.4/164.9 kB 369.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- 164.9/164.9 kB 353.2 kB/s eta 0:00:00\n",
      "Installing collected packages: striprtf, pypdf, certifi, openai, llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "  Attempting uninstall: pypdf\n",
      "    Found existing installation: pypdf 5.0.1\n",
      "    Uninstalling pypdf-5.0.1:\n",
      "      Successfully uninstalled pypdf-5.0.1\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.6.2\n",
      "    Uninstalling certifi-2024.6.2:\n",
      "      Successfully uninstalled certifi-2024.6.2\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.52.0\n",
      "    Uninstalling openai-1.52.0:\n",
      "      Successfully uninstalled openai-1.52.0\n",
      "Successfully installed certifi-2024.12.14 llama-cloud-0.1.7 llama-index-0.12.8 llama-index-agent-openai-0.4.1 llama-index-cli-0.4.0 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.3 llama-index-llms-openai-0.3.12 llama-index-multi-modal-llms-openai-0.4.1 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.1 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.19 openai-1.58.1 pypdf-5.1.0 striprtf-0.0.26\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.32.0 requires protobuf<5,>=3.20, but you have protobuf 5.28.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8b676a3e-3326-45a9-a714-8ba66ac6cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document,ServiceContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.readers.microsoft_onedrive import OneDriveReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "330d1324-062f-48b8-8380-1fa0f2e3a5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:llama_index.readers.microsoft_onedrive.base:An error occurred while loading the data: API request to download /AmirEjaz failed with status code: 403, message: b'{\"error\":{\"code\":\"Authorization_RequestDenied\",\"message\":\"Insufficient privileges to complete the operation.\",\"innerError\":{\"date\":\"2024-12-28T09:59:25\",\"request-id\":\"b3f93286-b038-489c-8c14-590d694bae1e\",\"client-request-id\":\"b3f93286-b038-489c-8c14-590d694bae1e\"}}}'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Abbas\\anaconda3\\Lib\\site-packages\\llama_index\\readers\\microsoft_onedrive\\base.py\", line 637, in load_data\n",
      "    payloads = self._get_downloaded_files_metadata(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Abbas\\anaconda3\\Lib\\site-packages\\llama_index\\readers\\microsoft_onedrive\\base.py\", line 599, in _get_downloaded_files_metadata\n",
      "    return self._init_download_and_get_metadata(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Abbas\\anaconda3\\Lib\\site-packages\\llama_index\\readers\\microsoft_onedrive\\base.py\", line 494, in _init_download_and_get_metadata\n",
      "    payload = self._connect_download_and_return_metadata(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Abbas\\anaconda3\\Lib\\site-packages\\llama_index\\readers\\microsoft_onedrive\\base.py\", line 394, in _connect_download_and_return_metadata\n",
      "    data = self._get_items_in_drive_with_maxretries(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Abbas\\anaconda3\\Lib\\site-packages\\llama_index\\readers\\microsoft_onedrive\\base.py\", line 240, in _get_items_in_drive_with_maxretries\n",
      "    raise Exception(\n",
      "Exception: API request to download /AmirEjaz failed with status code: 403, message: b'{\"error\":{\"code\":\"Authorization_RequestDenied\",\"message\":\"Insufficient privileges to complete the operation.\",\"innerError\":{\"date\":\"2024-12-28T09:59:25\",\"request-id\":\"b3f93286-b038-489c-8c14-590d694bae1e\",\"client-request-id\":\"b3f93286-b038-489c-8c14-590d694bae1e\"}}}'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from llama_index.readers.microsoft_onedrive import OneDriveReader\n",
    "\n",
    "# Initialize the OneDriveReader\n",
    "onedrive_reader = OneDriveReader(\n",
    "    client_id=\"be9fab77-1cd5-45c1-91f3-81371dfae656\",\n",
    "    tenant_id=\"178774fc-26a7-49ed-977e-0feb6a7d866b\",\n",
    "    client_secret=\"~jw8Q~BhZBf~jGVA1rVXfFeBRrijZRneNWcbwdoA\",\n",
    "    userprincipalname=\"talhahashmi940_gmail.com#EXT#@talhahashmi940gmail.onmicrosoft.com\",\n",
    "    folder_path=\"/AmirEjaz\"  # Added leading slash\n",
    ")\n",
    "\n",
    "# Load documents from OneDrive\n",
    "try:\n",
    "    documents = onedrive_reader.load_data()\n",
    "    print(documents)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
